<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="TARGET Benchmark for Evaluating Table Retrieval in RAG Pipelines">
  <meta property="og:title" content="TARGET Benchmark"/>
  <meta property="og:description" content="TARGET Benchmark for Evaluating Table Retrieval in RAG Pipelines"/>
  <meta property="og:url" content="https://target-benchmark.github.io"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/target_img.png" />
  <meta property="og:image:width" content="630"/>
  <meta property="og:image:height" content="630"/>


  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="table retrieval, benchmark, RAG, QA, text-to-sql">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>TARGET: Benchmarking Table Retrieval for Generative Tasks</title>
  <link rel="icon" type="image/x-icon" href="static/images/target_img.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-LNNSB6SB2V"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-LNNSB6SB2V');
  </script>

</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">&#127919; TARGET: Benchmarking Table Retrieval for Generative Tasks</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">Xingyu Ji,</span>
                <span class="author-block"> <a href="https://people.eecs.berkeley.edu/~adityagp/" target="_blank">Aditya Parameswaran</a>,</span>
              <span class="author-block"> <a href="https://www.madelonhulsebos.com" target="_blank">Madelon Hulsebos</a><sup>*</sup></span>
              </div>

            <div class="is-size-5 publication-authors">
                <span class="author-block">UC Berkeley</span>
                <span class="eql-cntrb"><small><br><sup>*</sup>Now at CWI</small></span>
               </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="/static/pdfs/TARGET_V1.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://huggingface.co/target-benchmark" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        &#129303;
                      </span>
                      <span>HuggingFace</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/target-benchmark/target" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
<!--                 <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                <span>arXiv</span> -->
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/target_diagram.png" alt="Overview diagram of the TARGET benchmark for evaluating table retrieval for generative tasks"/>
<!--       <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
<!--         <source src="static/videos/banner_video.mp4"
        type="video/mp4"> -->
<!--       </video> -->
      <h2 class="subtitle has-text-centered">
       Overview of the TARGET benchmark.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The data landscape is rich with structured data, often of high value to organizations, driving important applications in data analysis and machine learning.
            Recent progress in representation learning and generative models for such data has led to the development of natural language interfaces to structured data,
            including those leveraging text-to-SQL. Contextualizing interactions, either through conversational interfaces or agentic components, in structured data through
            retrieval-augmented generation can provide substantial benefits in the form of freshness, accuracy, and comprehensiveness of answers.
            The key question is: how do we retrieve the right table(s) for the analytical query or task at hand? To this end, we introduce TARGET: a benchmark for evaluating <b>TA</b>ble <b>R</b>etrieval for <b>GE</b>nerative <b>T</b>asks.
            With TARGET we analyze the retrieval performance of different retrievers in isolation, as well as their impact on downstream tasks.
            We find that dense embedding-based retrievers far outperform a BM25 baseline which is less effective than it is for retrieval over unstructured text.
            We also surface the sensitivity of retrievers across various metadata (e.g., missing table titles), and demonstrate a stark variation of retrieval performance across datasets and tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<sectio class="section"n>
  <div class="container is-max-desktop">
      <h2 class="title">The TARGET Benchmark</h2>

      <b>What is TARGET?</b>
      <br>
      TARGET is the first benchmark for evaluating open-domain querying over tabular data as illustrated in the below figure. TARGET enables consistent and comprehensive evaluation of models and pipelines for table retrieval in isolation, as well as end-to-end for downstream tasks (question answering, fact verification, and text-to-SQL).
      In our paper, we use TARGET to analyze retrieval methods based on sparse lexical representations, dense embeddings of metadata, dense table embeddings, and dense row embedding.<br>
      
      <br>
      The curated datasets and the TARGET package (see above links) are designed for easy reuse for custom systems for RAG with tabular data.
      <br>
      <br>

      <img src="static/images/retrieval-pipeline.png" alt="Pipeline of open domain question answering over tabular data, in which no tables for the question are provided but are needed to be retrieved first."/><br>

      <br>
      <br>
      <b>Why TARGET?</b>
      <li> Existing systems and benchmarks for retrieval-augmented generation (RAG) pipelines mainly focus on retrieval of text, images, and audio data. The value of tabular data for RAG is overlooked, while tables typically contain fresh, reliable and domain-specific data. As we show in our paper, grounding LLM dialogues in tabular data significantly improve accuracy.</li>
      <li> We connect LLM-powered analytical querying over structured data (e.g. through table lookups or SQL generation) with a retriever, extending ``closed domain'' querying systems (e.g. through table reasoning or SQL generation) to the ``open domain'' setting.</li>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
      <h2 class="title">Key findings</h2>

    <b>Retrieval</b><br>
    
    We find that table retrieval based on sparse lexical representations such as BM25 (OTTQA) are less effective, across tasks and datasets, than they are for text.
    The importance of descriptive metadata for retrievers based on lexical representations is clear from their low performances on FeTaQA and TabFact, which do not contain descriptive metadata.
    LLM-generated table summaries with dense metadata embeddings can significantly improve retrieval performance as illustrated by the Dense Metadata Embedding baseline.
    Generally, Dense Table Embeddings (table header + rows) generally yield the best performance. Notably, for both text-to-SQL datasets, the effect of including data rows is minimal, with differences within ±5% in recall.
    The Dense Row-level Embedding method exhibits comparable performances to dense embeddings of tables with sampled rows, but becomes impractical for large tables, such as in BIRD. Further analysis regarding scaling, context limits, and effect of retrieval, can be found in the paper.

    <br>
    <br>
    <img src="static/images/retrieval_results_target.png" alt="Retrieval results of the TARGET benchmark for various retrievers, tasks and datasets"/><br>

    <br>
    <b>Generation</b><br>

    In general, the ``No Context'' baseline performs significantly lower without having relevant tables provided, illustrating the value of table retrieval (e.g. from Wikipedia) for grounding LLMs.
    The low performance of all retrievers on the OTTQA dataset is notable, which we hypothesize is due to the relatively short answers in OTTQA versus longer generated answers despite prompting for concisenes, and illustrates the need for more robust evaluation metrics.
    Naturally, due to the stronger retriever performance of dense embeddings, we find that dense retrievers generally yield best downstream performance across datasets. Meanwhile, the poor retrieval performance of sparse lexical representations on FeTaQA seems to distract the generator with irrelevant tables.
    To understand the limitations of LLM context for table comprehension tasks, we explore the relationship between the rank of the ground-truth table in the retrieval results and downstream task performance and find that the accuracy significantly decreases when the correct table is lower positioned in context (more details are in the paper).
    <br>
    <br>
    <img src="static/images/generation_results_target.png" alt="Generation results of the TARGET benchmark for various retrievers, tasks and datasets"/>
 
  </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
      <!-- <div class="hero-body"> -->
        <h2 class="title">Contact</h2>
        We warmly welcome contributions and suggestions for TARGET, please find instructions at: <a href="https://github.com/target-benchmark/target" target="_blank">https://github.com/target-benchmark/target</a>.
        Want to share/discuss something else, please reach out to Madelon Hulsebos (<a href="mailto:madelon@cwi.nl" target="_blank">madelon@cwi.nl</a>)!
    </div>
</section>


<!--BibTex citation -->
<section class="section" id="BibTex">
    <div class="container is-max-desktop">
      <h2 class="title">Citation</h2>
        <pre><code>@inproceedings{ji2024target,
  title={TARGET: Benchmarking Table Retrieval for Generative Tasks},
  author={Ji, Xingyu and Parameswaran, Aditya and Hulsebos, Madelon},
  booktitle={NeurIPS 2024 Third Table Representation Learning Workshop}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

  </body>
  </html>
